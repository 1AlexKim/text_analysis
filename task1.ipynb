{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-3) Загрузим датасет, считаем  в Python. Подготовим для дальнейшей работы два списка: список текстов в порядке их следования в датасете и список соответствующих им меток классов. В качестве метки класса используйте 1 для спама и 0 для \"не спама\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('SMSSpamCollection.txt', dtype=None, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = [( i[1].decode('utf8')) for i in data]\n",
    "target = [(0 if i[0].decode('utf8') == 'ham' else 1) for i in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Используя sklearn.feature_extraction.text.CountVectorizer со стандартными настройками, получим из списка текстов матрицу признаков X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvr = CountVectorizer()\n",
    "X = cvr.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Оценим качество классификации текстов с помощью LogisticRegression() с параметрами по умолчанию, используя sklearn.cross_validation.cross_val_score и посчитав среднее арифметическое качества на отдельных fold'ах. Параметр cv зададим равным 10. В качестве метрики качества используем f1-меру класса 1 (то есть, без микро и макро усреднения). Получившееся качество – ответ в этом пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.933419572279\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=1)\n",
    "scores = cross_val_score(logreg, X, target, scoring='f1', cv=10)\n",
    "print('Mean score:', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Обучим классификатор на всей выборке и спрогнозируем с его помощью класс для следующих сообщений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text1 = [\n",
    "    \"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "    \"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "    \"Have you visited the last lecture on physics?\", \n",
    "    \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\", \n",
    "    \"Only 99$\"\n",
    "]\n",
    "logreg.fit(X, target)\n",
    "result = logreg.predict(cvr.transform(text1))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93341957227863492, 0.81857220331211278, 0.72396573537867059, 0.92362874602119605]\n"
     ]
    }
   ],
   "source": [
    "logreg_ngram_scores = []\n",
    "for ng_range in [(1,1),(2,2), (3,3), (1,3)]:\n",
    "    vect = CountVectorizer(ngram_range=ng_range)\n",
    "    X = vect.fit_transform(text)\n",
    "    logreg_ngram_scores.append(cross_val_score(logreg, X, target, scoring='f1', cv=10).mean())\n",
    "\n",
    "print([i for i in logreg_ngram_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) тоже самое для  MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92767639962595572, 0.64493971901126523, 0.38059278664201324, 0.89071678099186435]\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "\n",
    "MNB_ngram_scores = []\n",
    "for ng_range in [(1,1),(2,2), (3,3), (1,3)]:\n",
    "    vect = CountVectorizer(ngram_range=ng_range)\n",
    "    X = vect.fit_transform(text)\n",
    "    MNB_ngram_scores.append(cross_val_score(MNB, X, target, scoring='f1', cv=10).mean())\n",
    "\n",
    "\n",
    "print([i for i in MNB_ngram_scores])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Tfidfvectorizer на униграммах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.852662726124\n"
     ]
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(ngram_range=(1,1))\n",
    "X = tfidfv.fit_transform(text)\n",
    "print('Mean score:', cross_val_score(logreg, X, target, scoring='f1', cv=10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ниже, чем с CountVectorizer() на униграммах(0.85 < 0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Попробуем получить как можно более высокое качество на кросс-валидации(используем токенизацию, стэмминг и удаление стоп-слов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сначала Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'C': array([  1.00000e-02,   1.00000e-01,   5.00000e-01,   1.00000e+00,\n",
      "         5.00000e+00,   1.00000e+01,   1.00000e+02,   2.00000e+02,\n",
      "         5.00000e+02,   1.00000e+03,   1.00000e+04,   1.50000e+04,\n",
      "         2.00000e+04,   1.00000e+05])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "\n",
      "\n",
      "Best score = 0.980086114101\n",
      "\n",
      "\n",
      "значение коэффициента С 100000.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "X = []\n",
    "for i in range(len(text)):\n",
    "    X.append(((tokenizer.tokenize(text[i]))))\n",
    "    for j in range(len(X[i])):\n",
    "        X[i][j] = PorterStemmer().stem(X[i][j])\n",
    "    X[i] = \" \".join(X[i])\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "X = tfidf.fit_transform(X)\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array( [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = LogisticRegression()\n",
    "# model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(C=alphas), cv=5)\n",
    "grid.fit(X, target)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print('\\n')\n",
    "print('Best score =', grid.best_score_)\n",
    "print('\\n')\n",
    "print('значение коэффициента С', grid.best_estimator_.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'C': array([  1.00000e-02,   1.00000e-01,   5.00000e-01,   1.00000e+00,\n",
      "         5.00000e+00,   1.00000e+01,   1.00000e+02,   2.00000e+02,\n",
      "         5.00000e+02,   1.00000e+03,   1.00000e+04,   1.50000e+04,\n",
      "         2.00000e+04,   1.00000e+05])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "\n",
      "\n",
      "Best score = 0.983494797273\n",
      "\n",
      "\n",
      "значение коэффициента С 20000.0\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for i in range(len(text)):\n",
    "    X.append(((tokenizer.tokenize(text[i]))))\n",
    "    for j in range(len(X[i])):\n",
    "        X[i][j] = PorterStemmer().stem(X[i][j])\n",
    "    X[i] = \" \".join(X[i])\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "X = vect.fit_transform(text)\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array( [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = LogisticRegression()\n",
    "# model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(C=alphas), cv=5)\n",
    "grid.fit(X, target)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print('\\n')\n",
    "print('Best score =', grid.best_score_)\n",
    "print('\\n')\n",
    "print('значение коэффициента С', grid.best_estimator_.C)\n",
    "vect = CountVectorizer(ngram_range=ng_range)\n",
    "X = vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Для задачи бинарной классификации текстов SMS-сообщений CountVectorizer лучше подходит в качестве способа отбора признаков, нежели TfidfVectorizer.\n",
    "\n",
    "2) При выборе других моделей(не лог-регрессии) результаты ухудшались\n",
    "\n",
    "3) Предобработка текста идёт на пользу качеству классификации)\n",
    "\n",
    "4)  n-граммы не помогли"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
